{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on TMDB Dataset Vote Average Classification\n",
    "\n",
    "This is a machine learning project that will consist on predicting different things about the TMDB Datasets. The objective of this notebook is to use machine learning.\n",
    "\n",
    "Some columns consist of non-numerical data and could be very useful when finding patterns\n",
    "between corresponding data. Therefore encoding these features is necessary if you want to\n",
    "use them in a model. Features such as genres and production companies contain lists of\n",
    "elements; through the use of sklearnâ€™s preprocessing libraries, multilabelbinarizer helps\n",
    "convert the information in the lists to a one hot version. This does however create new\n",
    "columns for each unique label, potentially increasing the size of the dataframe by a large\n",
    "amount and heavily impacting the training time. \n",
    "\n",
    "We had the idea to use all the number of columns created by the genres using on-hot encoding and then use pca to reduce the number of columns or dimensions, and then used them for predicting or input them into the neural network.However, given the time, we could not do that. \n",
    "\n",
    "For this reason, when using any model\n",
    "containing cast or crew we will limit them to the first five elements in an attempt to keep the\n",
    "dataset uncluttered.\n",
    "\n",
    "### Predicting vote average based on classification.\n",
    "\n",
    "For vote average we deleted rows where the number was zero.\n",
    "We predicted vote average based on revenue, popularity and vote_count, and genres. We had to encode the genres, to find meaningful relationships, the original data was on a json format where we have to format it into an array or genres, and then we used an encoding to encode the genres, as taking all of them would create a lot of columns columns. \n",
    "\n",
    "We did tried different methods that were for classification such as KNN, Random Forest and Gaussian Naive Bayes Gaussian Mixtures with support vector machines and neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "This is the plan that we are going to follow.\n",
    "\n",
    "1. read csv file\n",
    "2. remove columns that have little correlation or we don't need.\n",
    "3. find if there are null values, dealing either by filling them it using media, average or in worst case dropping them.\n",
    "4. Finding X and y, and use crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  the dataset \n",
    "\n",
    "We are going to load the dataset by merging the two files by movie id.\n",
    "Then we will remove the columns that are not necesary to our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Non-numerical Features \n",
    "\n",
    "\n",
    "We are going to encode the different non numerical features such as crew, cast and genres.\n",
    "We are going to use one-hot encoding, however this will create many columns, so we will make the first ten rows.\n",
    "\n",
    "Before we actually can encode the data, we first need to conver the json data from the dataset to an appropiate format that we can use to use one-hot encoding.\n",
    "\n",
    "The convert() method below, converts the data from the dataframe that is in json format to a list of python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_list(dataf, feature):\n",
    "    enc_df = dataf.join(pd.DataFrame.sparse.from_spmatrix(mlb.fit_transform(temp.pop(feature)),\n",
    "                                                     index = dataf.index,\n",
    "                                                     columns = mlb.classes_))\n",
    "    return enc_df\n",
    "\n",
    "def convert(df, columns): \n",
    "    for c in columns:\n",
    "        # Convert json format to python list\n",
    "        df[c]=df[c].apply(json.loads)\n",
    "        \n",
    "        # Obtain first 10 from columns cast and crew\n",
    "        if (c == 'cast' or 'crew' or 'production_companies'): \n",
    "            for index,i in zip(df.index,df[c]):\n",
    "                limit = 5\n",
    "                if len(i) < 5:\n",
    "                    limit = len(i)\n",
    "                \n",
    "                temp_list=[]\n",
    "                for j in range(limit):\n",
    "                    # Json format of 'id' & 'name'\n",
    "                    temp_list.append((i[j]['name'])) \n",
    "                df.loc[index,c]= str(temp_list)\n",
    "\n",
    "        # For any other columns\n",
    "        else:    \n",
    "            for index,i in zip(df.index,df[c]):\n",
    "                temp_list=[]\n",
    "                for j in range(len(i)):\n",
    "                    temp_list.append((i[j]['name'])) \n",
    "                df.loc[index,c]= str(temp_list)\n",
    "    \n",
    "         \n",
    "        df[c] = df[c].str.strip('[]')       # Remove Sqr Brackets\n",
    "        df[c] = df[c].str.replace(' ','')   # Remove empty space \n",
    "        df[c] = df[c].str.replace(\"'\",'')   # Remove quotations\n",
    "        df[c] = df[c].str.split(',')        # Format into list\n",
    "        \n",
    "        # Sort elements \n",
    "        for i,j in zip(df[c],df.index):\n",
    "            temp_list = i\n",
    "            temp_list.sort()\n",
    "            df.loc[j,c]=str(temp_list)\n",
    "            \n",
    "        df[c] = df[c].str.strip('[]')       \n",
    "        df[c] = df[c].str.replace(' ','')    \n",
    "        df[c] = df[c].str.replace(\"'\",'')   \n",
    "       \n",
    "        lst = df[c].str.split(',')        \n",
    "        if len(lst) == 0:\n",
    "            df[c] = None\n",
    "        else:\n",
    "            df[c]= df[c].str.split(',')\n",
    "            \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading  the dataset \n",
    "\n",
    "We are going to load the dataset by merging the two files by movie id.\n",
    "Then we will remove the columns that are not necesary to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./\"\n",
    "filename_read = os.path.join(path1,\"tmdb_5000_movies.csv\")\n",
    "movie = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "path2 = \"./\"\n",
    "filename_read = os.path.join(path2,\"tmdb_5000_credits.csv\")\n",
    "credit = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "movies = movie.merge(credit, left_on='id', right_on='movie_id', how='left')\n",
    "\n",
    "movies = movies.drop(columns=['homepage','original_language','title_y', 'title_x',\n",
    "                              'overview','production_countries','release_date',\n",
    "                              'spoken_languages','status','tagline'\n",
    "                              ], axis=1) # , 'runtime', , 'genres'\n",
    "\n",
    "\n",
    "test = convert(movies,  ['genres', 'keywords', 'production_companies', 'cast', 'crew'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data to predict classification\n",
    "\n",
    "We are cleasing the data for classification and removing entries with score of 0.\n",
    "We then tey to delete rows containing empty values from cast, crew, companies columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing entries with empty cast/crew/companies\n",
    "drop = []\n",
    "for i in test.index:\n",
    "    if (test['production_companies'][i] == [''] and test['cast'][i] == [''] and \n",
    "       test['crew'][i] == ['']): \n",
    "        drop.append(i)\n",
    "test = test.drop(drop, axis = 0)\n",
    "\n",
    "# Removing entries with a score of 0\n",
    "mask_avg = (test['vote_average'] != 0)\n",
    "test = test[mask_avg]\n",
    "temp = test.copy()\n",
    "\n",
    "test.shape\n",
    "test.dtypes\n",
    "\n",
    "temp['vote_round'] = temp.vote_average.round()\n",
    "temp = temp.drop(columns = 'vote_average')\n",
    "\n",
    "countCol = temp.groupby(\"vote_round\")[\"vote_round\"].transform(len)\n",
    "mask = (countCol >= 10)\n",
    "temp = temp[mask]\n",
    "\n",
    "temp.vote_round.value_counts()\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True) \n",
    "enc_df = encode_list(temp, feature = 'genres')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoded = enc_df.copy()\n",
    "\n",
    "encoded.dtypes\n",
    "encoded.shape\n",
    "# Remove unecessary columns\n",
    "encoded = encoded.drop(columns=['id', 'keywords', 'original_title','movie_id',\n",
    "                                'production_companies', 'cast', 'crew', 'runtime']) \n",
    "encoded.isnull().any()\n",
    "# Converting datatypes for encoded genres \n",
    "for i in range(5, 26):\n",
    "    name = encoded.columns[i]\n",
    "    encoded[name] = np.asarray(encoded[name]).astype('float32')\n",
    "\n",
    "U = encoded.drop(columns=['vote_round'])\n",
    "v = encoded['vote_round'].astype('int32')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(v)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_v = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "U.shape\n",
    "v.shape\n",
    "onehot_v.shape\n",
    "onehot_v[:10]\n",
    "v[:10]\n",
    "\n",
    "encoded['vote_round'].value_counts()\n",
    "\n",
    "v_flat = v.values.ravel()\n",
    "v_flat = to_categorical(v_flat) \n",
    "v_flat.shape\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(U, onehot_v, test_size=0.25, random_state=5)\n",
    "\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "\n",
    "X_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import  Flatten\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## NEURAL NETWORK ###################################################\n",
    "\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='loss', patience=2),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='loss', save_best_only=True)]\n",
    "\n",
    "opt = SGD(learning_rate = 0.01, momentum=0.9, decay=0.01)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim = U.shape[1], activation = 'sigmoid')) #Input Layer -> Hidden Layer 1\n",
    "model.add(Dense(17, activation = 'sigmoid')) # Hidden Layer 1 -> Hidden Layer 2\n",
    "model.add(Dense(7,  activation = 'softmax')) # Hidden Layer 2 -> Output Layer\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy']) # Choosing Loss & Optimizer \n",
    "net = model.fit(X_train, y_train, verbose = 2, epochs = 200)#, callbacks = callbacks, batch_size = 64) # Training data\n",
    "pred = model.predict(X_test)  # make predictions \n",
    "pred = np.argmax(pred,axis=1) # now pick the most likely outcome\n",
    "y_compare = np.argmax(y_test,axis=1) \n",
    "\n",
    "#calculate accuracy\n",
    "score = metrics.accuracy_score(y_compare, pred) \n",
    "print(\"Accuracy score: {}\".format(score))\n",
    "\n",
    "for key, value in net.history.items() :\n",
    "    print (key)\n",
    "    \n",
    "plt.plot(net.history['loss'])\n",
    "#plt.plot(net.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(net.history['accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylabel('accruacy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accruacy'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "cm = confusion_matrix(y_compare, pred)\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()\n",
    "pred[:10]\n",
    "\n",
    "y_test.shape\n",
    "print(y_test[:10])\n",
    "print(pred[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Classification\n",
    "                                    \n",
    "# Doesn't work too well with higher dimensiosn as it is more difficult to calculate distances in high dimensions\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(algorithm = 'auto',           # Find the best algorithm \n",
    "                            metric = 'minkowski',        # Distance Metric E.g. Minkowski\n",
    "                            n_neighbors = 7,             # \n",
    "                            p = 2,                       # Manhattan (1) / Euclidean (2) -  Distance \n",
    "                            weights = 'distance')        # distance weighted points \n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "#print(\"Predictions form the classifier:\")\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "                                    # Random Forest Classifier \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(criterion = 'entropy', max_features = 'auto',\n",
    "                             n_estimators = 100, n_jobs = (4), oob_score=(True),\n",
    "                             max_depth=(30)) #min_samples_leaf = 2)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                                    \n",
    "U = encoded.drop(columns=['vote_round'])\n",
    "v = encoded['vote_round'].astype('int32')\n",
    "# run this for naive bayes \n",
    "X_train, X_test, y_train, y_test = train_test_split(U, v, test_size=0.25, random_state=3)\n",
    " \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(covariance_type = 'tied', n_components= 2, n_init = 10,\n",
    "                      verbose = 2, verbose_interval = 5, random_state=4).fit(U)\n",
    "proba = gmm.predict_proba(U)\n",
    "svm = SVC().fit(proba, v)\n",
    "y_pred = svm.predict(U)\n",
    "print(\"Accuracy Score: \", metrics.accuracy_score(v, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
